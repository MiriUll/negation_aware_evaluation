{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Many thanks to [this Guthub issue](https://github.com/huggingface/datasets/issues/224) and the [resulting repository](https://github.com/lucadiliello/bleurt-pytorch)\n",
    "### This notebook is an extended copy of [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Option 1: Install bleurt repository\n",
    "!pip install --upgrade pip  # ensures that pip is current\n",
    "!git clone https://github.com/google-research/bleurt.git\n",
    "!pip install ./bleurt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Option 2: Clone repository from https://github.com/google-research/bleurt and append it to path\n",
    "import sys\n",
    "sys.path.append(\"../bleurtMaster\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import bleurt\n",
    "from bleurt import score as bleurt_score\n",
    "import sys\n",
    "sys.argv = sys.argv[:1] ##thanks https://github.com/google-research/bleurt/issues/4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## Step 1: Convert model to torch\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import torch\n",
    "\n",
    "checkpoint = \"../neg_bleurt_500\" #path to saved bleurt model\n",
    "imported = tf.saved_model.load_v2(checkpoint)\n",
    "\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "\n",
    "class BleurtModel(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert = transformers.BertModel(config)\n",
    "        self.dense = nn.Linear(config.hidden_size,1)\n",
    "\n",
    "    def forward(self, input_ids, input_mask, segment_ids):\n",
    "        cls_state = self.bert(input_ids, input_mask,\n",
    "                            #   segment_ids)[0][:,0]#[1] doesnt work either\n",
    "                              segment_ids).pooler_output # this is fix #2 - taking pooler output\n",
    "        return self.dense(cls_state)\n",
    "\n",
    "state_dict = {}\n",
    "for variable in imported.variables:\n",
    "    n = variable.name\n",
    "    if n.startswith('global'):\n",
    "        continue\n",
    "    data = variable.numpy()\n",
    "    # if 'dense' in n:\n",
    "    if 'kernel' in n:  # this is fix #1 - considering 'kernel' layers instead of 'dense'\n",
    "        data = data.T\n",
    "    n = n.split(':')[0]\n",
    "    n = n.replace('/','.')\n",
    "    n = n.replace('_','.')\n",
    "    n = n.replace('kernel','weight')\n",
    "    if 'LayerNorm' in n:\n",
    "        n = n.replace('beta','bias')\n",
    "        n = n.replace('gamma','weight')\n",
    "    elif 'embeddings' in n:\n",
    "        n = n.replace('word.embeddings','word_embeddings')\n",
    "        n = n.replace('position.embeddings','position_embeddings')\n",
    "        n = n.replace('token.type.embeddings','token_type_embeddings')\n",
    "        n = n + '.weight'\n",
    "    state_dict[n] = torch.from_numpy(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g3rWgezAUtGB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "config = transformers.BertConfig(hidden_size= 128, hidden_act= \"gelu\", initializer_range= 0.02, vocab_size= 30522, hidden_dropout_prob= 0.1, num_attention_heads= 2, type_vocab_size= 2, max_position_embeddings= 512, num_hidden_layers= 2, intermediate_size= 512, attention_probs_dropout_prob= 0.1)\n",
    "bleurt_model = BleurtModel(config)\n",
    "bleurt_model.load_state_dict(state_dict, strict=False)  # strict=False added otherwise crashes.\n",
    "# Should be safe, according to this https://github.com/huggingface/transformers/issues/6882#issuecomment-884730078\n",
    "for param in bleurt_model.parameters():\n",
    "    param.requires_grad = False\n",
    "bleurt_model.eval()\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "config = transformers.BertConfig(hidden_size= 128, hidden_act= \"gelu\", initializer_range= 0.02, vocab_size= 30522, hidden_dropout_prob= 0.1, num_attention_heads= 2, type_vocab_size= 2, max_position_embeddings= 512, num_hidden_layers= 2, intermediate_size= 512, attention_probs_dropout_prob= 0.1, num_labels=1)\n",
    "bleurt_model = BertForSequenceClassification(config)\n",
    "state_dict['classifier.weight'] = state_dict.pop('dense.weight')\n",
    "state_dict['classifier.bias'] = state_dict.pop('dense.bias')\n",
    "bleurt_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "bleurt_model.save_pretrained(\"negBLEURT\") # Note: this saves a pytorch model but is missing the tokenizer info"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "data": {
      "text/plain": "('negBLEURT\\\\tokenizer_config.json',\n 'negBLEURT\\\\special_tokens_map.json',\n 'negBLEURT\\\\vocab.txt',\n 'negBLEURT\\\\added_tokens.json',\n 'negBLEURT\\\\tokenizer.json')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 2: Create a tokenizer\n",
    "\n",
    "import json\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "with open(f'{checkpoint}/bleurt_config.json','r') as f:\n",
    "    bleurt_config = json.load(f)\n",
    "\n",
    "max_seq_length = bleurt_config[\"max_seq_length\"]\n",
    "vocab_file = f'{checkpoint}/{bleurt_config[\"vocab_file\"]}'\n",
    "do_lower_case = bleurt_config[\"do_lower_case\"]\n",
    "\n",
    "tokenizer = bleurt.lib.tokenizers.create_tokenizer(\n",
    "    vocab_file=vocab_file, do_lower_case=do_lower_case, sp_model=None)\n",
    "\n",
    "mytok = BertTokenizerFast(vocab_file=vocab_file, do_lower_case=do_lower_case, max_seq_length=max_seq_length)\n",
    "mytok.save_pretrained(\"negBLEURT\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Dh8btlLeg4Qp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "26019884-3af1-40c9-95c1-d9e03acd26bb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Step 3: Compare model outputs of Pytorch transformer model and BleurtScorer\n",
    "references = [\"a bird chirps by the window\", \"This is a test.\"]\n",
    "candidates = [\"a bird chirps by the window\", \"This isn't a test.\"]\n",
    "\n",
    "scorer = bleurt_score.BleurtScorer(checkpoint)\n",
    "scores = scorer.score(references=references, candidates=candidates)\n",
    "print(scores)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint neg_bleurt_500.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... sp_model:None\n",
      "INFO:tensorflow:... dynamic_seq_length:False\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n",
      "[0.9107678532600403, 0.12219361960887909]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aULTeApxga5D",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8234794f-4c65-4c22-a3af-1ded8ec62650",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "bleurt_tok = AutoTokenizer.from_pretrained(\"negBLEURT\")\n",
    "bleurt_model = AutoModelForSequenceClassification.from_pretrained(\"negBLEURT\")\n",
    "\n",
    "encoding = bleurt_tok(references, candidates, padding=True, return_tensors='pt')\n",
    "bleurt_model.eval()\n",
    "bleurt_model(**encoding).logits.flatten().tolist()"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.9107482433319092, 0.12223134934902191]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  }
 ]
}