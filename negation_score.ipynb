{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from sentence_transformers import models, losses, datasets\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence Transformer score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "timestamp = \"2023-02-17_15-02-13\"\n",
    "project_base_path = Path(\"Guided Research WS22\")\n",
    "negation_dataset = project_base_path / \"data/negation_dataset_labeled.tsv\"\n",
    "\n",
    "\n",
    "base_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "output_model_name = f\"{base_model.split('/')[1]}-negation\"  # TODO.\n",
    "model_save_path = str(project_base_path / f\"finetuned-models/{timestamp}/{output_model_name}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 75, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model = SentenceTransformer(model_save_path)\n",
    "base_model = SentenceTransformer(base_model)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "base_model.to(device)\n",
    "finetuned_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model score 0.6408535838127136\n",
      "Fine-tuned model score 0.3927837014198303\n",
      "\n",
      "\n",
      "Base model score tensor([0.6409, 0.8470])\n",
      "Fine-tuned model score tensor([0.3928, 0.5079])\n"
     ]
    }
   ],
   "source": [
    "def cos_score(reference: str, candidate: str, model:SentenceTransformer) -> float:\n",
    "    emb_ref = model.encode(reference)\n",
    "    emb_cand = model.encode(candidate)\n",
    "    return util.cos_sim(emb_ref, emb_cand).item()\n",
    "\n",
    "def cos_score_batched(references: list, candidates: list, model: SentenceTransformer, batch_size=8) -> torch.Tensor:\n",
    "    assert len(references) == len(candidates), \"Number of references and candidates must be equal\"\n",
    "    emb_ref = model.encode(references, batch_size=batch_size)\n",
    "    emb_cand = model.encode(candidates, batch_size=batch_size)\n",
    "    return torch.diag(util.cos_sim(emb_ref, emb_cand))\n",
    "\n",
    "sents1 = [\"It's rather hot in here.\", \"This is a red cat with a hat.\"]\n",
    "sents2 = [\"It's rather cold in here.\", \"This isn't a red cat with a hat.\"]\n",
    "print(\"Base model score\", cos_score(sents1[0], sents2[0], base_model))\n",
    "print(\"Fine-tuned model score\", cos_score(sents1[0], sents2[0], finetuned_model))\n",
    "print(\"\\n\")\n",
    "print(\"Base model score\", cos_score_batched(sents1, sents2, base_model))\n",
    "print(\"Fine-tuned model score\", cos_score_batched(sents1, sents2, finetuned_model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Seq2seq score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "model_dir = Path(\"Guided Research WS22/finetuned-models/010/flan-t5-negate/checkpoint-2000\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sents1 = [\"It's rather hot in here.\", \"This is a red cat with a hat.\"]\n",
    "sents2 = [\"It's rather cold in here.\", \"This isn't a red cat with a hat.\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It isn't rather hot in here.\", \"It's rather hot in here.\", \"It's not rather hot in here.\", \"It's rather cold in here.\", \"This isn't a red cat with a hat.\", 'This is not a red cat with a hat.', 'This is a black cat with a hat.', 'This is a white cat with a hat.']\n"
     ]
    }
   ],
   "source": [
    "inputs = [\"negate: \"+ sent for sent in sents1]\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True)\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    generation_config=GenerationConfig(\n",
    "        do_sample=False,\n",
    "        num_beams=4,\n",
    "        # penalty_alpha=0.5,\n",
    "        # top_k=10\n",
    "    ),\n",
    "    num_return_sequences=4\n",
    ")\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "# encode the inputs\n",
    "task_prefix = \"negate: \"\n",
    "\n",
    "encoding = tokenizer(\n",
    "    [task_prefix + sequence for sequence in sents1],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "# encode the targets\n",
    "target_encoding = tokenizer(sents2,\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "labels = target_encoding.input_ids\n",
    "\n",
    "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "# forward pass\n",
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "0.16529177129268646"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Eval on DEMETR data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def load_demetr_dataset(data_path:str) -> pd.DataFrame:\n",
    "    df:pd.DataFrame = pd.read_json(data_path)\n",
    "    return df\n",
    "\n",
    "demetr_data_path = \"demetr-main/dataset/\"\n",
    "perturbation_datasets = {\n",
    "    \"negation\": load_demetr_dataset(demetr_data_path + \"critical_id8_negation.json\"),\n",
    "    \"antonym\": load_demetr_dataset(demetr_data_path + \"critical_id7_antonym.json\"),\n",
    "    \"verb removed\": load_demetr_dataset(demetr_data_path + \"critical_id22_verb_removed.json\"),\n",
    "    \"hypernym\": load_demetr_dataset(demetr_data_path + \"major_id3_hypernym.json\")\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def demetr_accuracy(dataset: pd.DataFrame, model:SentenceTransformer, score_function) -> (float, np.array, np.array):\n",
    "    t_scores = score_function(dataset.eng_sent, dataset.mt_sent, model)\n",
    "    hat_scores = score_function(dataset.eng_sent, dataset.pert_sent, model)\n",
    "    return sum(torch.greater(t_scores, hat_scores)) / len(dataset), t_scores, hat_scores\n",
    "\n",
    "def demetr_ratio(dataset: pd.DataFrame, model:SentenceTransformer, score_function) -> None:\n",
    "    acc, t_scores, hat_scores = demetr_accuracy(dataset, model, score_function)\n",
    "    print(f\"Detection accuracy: {acc}\")\n",
    "    empty_scores = score_function(dataset.eng_sent, [\"\"] * len(dataset), model)\n",
    "    ratio = (t_scores - hat_scores) / (t_scores - empty_scores)\n",
    "    ratio = sum(ratio) / len(dataset)\n",
    "    print(f\"Ratio: {ratio}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*  Negation\n",
      "** Base model\n",
      "Detection accuracy: 0.972000002861023\n",
      "Ratio: 0.13973526656627655\n",
      "** Fine-tuned model\n",
      "Detection accuracy: 0.9929999709129333\n",
      "Ratio: 0.49932998418807983\n",
      "\n",
      "\n",
      "*  Antonym\n",
      "** Base model\n",
      "Detection accuracy: 0.925000011920929\n",
      "Ratio: 0.06956008076667786\n",
      "** Fine-tuned model\n",
      "Detection accuracy: 0.9459999799728394\n",
      "Ratio: 0.2047116905450821\n",
      "\n",
      "\n",
      "*  Verb removed\n",
      "** Base model\n",
      "Detection accuracy: 0.8149999976158142\n",
      "Ratio: 0.03189469873905182\n",
      "** Fine-tuned model\n",
      "Detection accuracy: 0.8140000104904175\n",
      "Ratio: 0.034169603139162064\n",
      "\n",
      "\n",
      "*  Hypernym\n",
      "** Base model\n",
      "Detection accuracy: 0.7850000262260437\n",
      "Ratio: 0.03764893859624863\n",
      "** Fine-tuned model\n",
      "Detection accuracy: 0.8069999814033508\n",
      "Ratio: 0.03380375728011131\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def eval_models_on_dataset(dataset:pd.DataFrame, score_function) -> None:\n",
    "    print(\"** Base model\")\n",
    "    demetr_ratio(dataset, base_model, score_function)\n",
    "    print(\"** Fine-tuned model\")\n",
    "    demetr_ratio(dataset, finetuned_model, score_function)\n",
    "\n",
    "for pert_name, pert_data in perturbation_datasets.items():\n",
    "    print(\"* \", pert_name.capitalize())\n",
    "    eval_models_on_dataset(pert_data, cos_score_batched)\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ True, False,  True])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = torch.tensor([0.5, 0.2, 0.7])\n",
    "b = torch.tensor([0.4, 0.4, 0.4])\n",
    "torch.greater(a,b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.2500, 0.1000, 0.3500])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0.5, 0.2, 0.7])\n",
    "b = torch.tensor([2, 2, 2])\n",
    "a/b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}