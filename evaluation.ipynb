{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unequal length arrays",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 21\u001B[0m\n\u001B[0;32m     15\u001B[0m new_score \u001B[38;5;241m=\u001B[39m new_score \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Perform two-sample t-test\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m#t_stat, p_value = ttest_ind(old_scores, [new_score])\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m#print(f\"t-statistic: {t_stat:.3f}\")\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m#print(f\"p-value: {p_value:.3f}\")\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m \u001B[43mttest_rel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mold_scores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mnew_score\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\users\\miriam anschütz\\documents\\workspaces\\pycharm projects\\negation_aware_evaluation\\venv\\lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:502\u001B[0m, in \u001B[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001B[1;34m(***failed resolving arguments***)\u001B[0m\n\u001B[0;32m    500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sentinel:\n\u001B[0;32m    501\u001B[0m     samples \u001B[38;5;241m=\u001B[39m _remove_sentinel(samples, paired, sentinel)\n\u001B[1;32m--> 502\u001B[0m res \u001B[38;5;241m=\u001B[39m hypotest_fun_out(\u001B[38;5;241m*\u001B[39msamples, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    503\u001B[0m res \u001B[38;5;241m=\u001B[39m result_to_tuple(res)\n\u001B[0;32m    504\u001B[0m res \u001B[38;5;241m=\u001B[39m _add_reduced_axes(res, reduced_axes, keepdims)\n",
      "File \u001B[1;32md:\\users\\miriam anschütz\\documents\\workspaces\\pycharm projects\\negation_aware_evaluation\\venv\\lib\\site-packages\\scipy\\stats\\_stats_py.py:7133\u001B[0m, in \u001B[0;36mttest_rel\u001B[1;34m(a, b, axis, nan_policy, alternative)\u001B[0m\n\u001B[0;32m   7131\u001B[0m nb \u001B[38;5;241m=\u001B[39m _get_len(b, axis, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msecond argument\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   7132\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na \u001B[38;5;241m!=\u001B[39m nb:\n\u001B[1;32m-> 7133\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munequal length arrays\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   7135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m nb \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   7136\u001B[0m     \u001B[38;5;66;03m# _axis_nan_policy decorator ensures this only happens with 1d input\u001B[39;00m\n\u001B[0;32m   7137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m TtestResult(np\u001B[38;5;241m.\u001B[39mnan, np\u001B[38;5;241m.\u001B[39mnan, df\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mnan, alternative\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mnan,\n\u001B[0;32m   7138\u001B[0m                        standard_error\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mnan, estimate\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mnan)\n",
      "\u001B[1;31mValueError\u001B[0m: unequal length arrays"
     ]
    }
   ],
   "source": [
    "# Significance test for antonym score\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Old scores\n",
    "old_scores = np.array([0.043, 0.15, 0.088])#, 0.098, 0.14, 0.044, 0.054])\n",
    "#old_scores = np.array([0.058, 0.21, 0.15, 0.15, 0.16, 0.053, 0.067])\n",
    "\n",
    "# New score\n",
    "new_score = 0.252\n",
    "#new_score = 0.6135\n",
    "\n",
    "old_scores = old_scores * 100\n",
    "new_score = new_score * 100\n",
    "\n",
    "# Perform two-sample t-test\n",
    "#t_stat, p_value = ttest_ind(old_scores, [new_score])\n",
    "#print(f\"t-statistic: {t_stat:.3f}\")\n",
    "#print(f\"p-value: {p_value:.3f}\")\n",
    "ttest_rel(old_scores, [new_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.005046185409119941 0.1923795187424533\n",
      "The improvement is significant.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Old scores\n",
    "old_scores = np.array([0.15, 0.043, 0.088])\n",
    "\n",
    "# New score\n",
    "new_score = 0.252\n",
    "\n",
    "# Calculate mean and standard deviation of old scores\n",
    "mean_old = np.mean(old_scores)\n",
    "std_old = np.std(old_scores, ddof=1)\n",
    "\n",
    "# Calculate t-value for 95% confidence interval with 3 degrees of freedom\n",
    "t_val = t.ppf(0.975, df=3)\n",
    "\n",
    "# Calculate confidence interval\n",
    "ci_low = mean_old - t_val * std_old / np.sqrt(len(old_scores))\n",
    "ci_high = mean_old + t_val * std_old / np.sqrt(len(old_scores))\n",
    "\n",
    "print(ci_low, ci_high)\n",
    "# Check if new score is outside of confidence interval\n",
    "if new_score < ci_low or new_score > ci_high:\n",
    "    print(\"The improvement is significant.\")\n",
    "else:\n",
    "    print(\"The improvement is not significant.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# Old scores\n",
    "#old_scores = np.array([0.043, 0.15, 0.088])#, 0.098, 0.14, 0.044, 0.054])\n",
    "old_scores = np.array([0.058, 0.21, 0.15, 0.15, 0.16, 0.053, 0.067])\n",
    "\n",
    "# New score\n",
    "#new_score = 0.252\n",
    "new_score = 0.6135\n",
    "# Concatenate old scores and new score\n",
    "all_scores = np.concatenate([old_scores, [new_score]])\n",
    "\n",
    "# Calculate observed difference\n",
    "obs_diff = np.mean(all_scores[:len(old_scores)]) - new_score\n",
    "\n",
    "# Permute labels and calculate difference many times\n",
    "n_perms = 100\n",
    "null_diffs = []\n",
    "for i in range(n_perms):\n",
    "    np.random.shuffle(all_scores)\n",
    "    null_diff = np.mean(all_scores[:len(old_scores)]) - all_scores[-1]\n",
    "    null_diffs.append(null_diff)\n",
    "\n",
    "# Calculate p-value\n",
    "p_value = sum(null_diffs >= obs_diff) / n_perms\n",
    "print(p_value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}